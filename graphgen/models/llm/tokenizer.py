from dataclasses import dataclass
from typing import List
import os
import re

# ç®€åŒ–çš„å†…ç½‘ç‰ˆæœ¬ï¼Œå»æ‰å¤æ‚çš„transformersä¾èµ–
# ä½¿ç”¨åŸºæœ¬çš„å­—ç¬¦è®¡æ•°å’Œåˆ†å‰²é€»è¾‘

class SimpleTokenizer:
    """ç®€åŒ–çš„tokenizerï¼Œé€‚ç”¨äºå†…ç½‘ç¯å¢ƒï¼Œæ— éœ€å¤æ‚çš„æ¨¡å‹ä¸‹è½½"""
    
    def __init__(self, chars_per_token: int = 4):
        """
        ç®€å•çš„tokenizerï¼ŒåŸºäºå­—ç¬¦æ•°ä¼°ç®—tokenæ•°é‡
        
        Args:
            chars_per_token: å¹³å‡æ¯ä¸ªtokençš„å­—ç¬¦æ•°ï¼ˆä¸­æ–‡çº¦2-3ï¼Œè‹±æ–‡çº¦4-5ï¼‰
        """
        self.chars_per_token = chars_per_token
    
    def encode(self, text: str) -> List[int]:
        """ç®€å•ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå­—ç¬¦ç´¢å¼•åˆ—è¡¨"""
        return [ord(c) for c in text]
    
    def decode(self, tokens: List[int]) -> str:
        """ç®€å•è§£ç ï¼šå°†å­—ç¬¦ç´¢å¼•åˆ—è¡¨è½¬æ¢å›æ–‡æœ¬"""
        return ''.join(chr(token) for token in tokens if 0 <= token <= 1114111)

def get_tokenizer(tokenizer_name: str = "simple"):
    """
    è·å–ç®€åŒ–çš„tokenizerå®ä¾‹
    å†…ç½‘ç‰ˆæœ¬ï¼šä½¿ç”¨ç®€å•çš„å­—ç¬¦åˆ†å‰²ï¼Œæ— éœ€ä¸‹è½½æ¨¡å‹
    """
    print(f"ğŸ”§ ä½¿ç”¨ç®€åŒ–tokenizer: {tokenizer_name}")
    return SimpleTokenizer()

@dataclass
class Tokenizer:
    """ç®€åŒ–çš„Tokenizerç±»ï¼Œé€‚ç”¨äºå†…ç½‘ç¯å¢ƒ"""
    model_name: str = "simple"

    def __post_init__(self):
        self.tokenizer = get_tokenizer(self.model_name)
        # åŸºäºå­—ç¬¦çš„ç®€å•tokenä¼°ç®—
        self.chars_per_token = 4  # å¹³å‡æ¯ä¸ªtoken 4ä¸ªå­—ç¬¦

    def encode_string(self, text: str) -> List[int]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºtokenåˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼šåŸºäºå­—ç¬¦æ•°ä¼°ç®—ï¼‰
        """
        # ç®€å•çš„tokenä¼°ç®—ï¼šå­—ç¬¦æ•°é™¤ä»¥å¹³å‡æ¯tokenå­—ç¬¦æ•°
        estimated_tokens = len(text) // self.chars_per_token
        return list(range(estimated_tokens))  # è¿”å›ç´¢å¼•åˆ—è¡¨

    def decode_tokens(self, tokens: List[int]) -> str:
        """
        å°†tokenåˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆï¼šç›´æ¥è¿”å›åŸæ–‡æœ¬çš„å‰Nä¸ªå­—ç¬¦ï¼‰
        """
        # ç”±äºæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–é€»è¾‘ï¼Œè¿™é‡Œè¿”å›ç©ºå­—ç¬¦ä¸²
        # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œåˆ†å—æ—¶ä¼šç›´æ¥ä½¿ç”¨åŸæ–‡æœ¬
        return ""

    def chunk_by_token_size(
        self, content: str, overlap_token_size=128, max_token_size=1024
    ):
        """
        æŒ‰tokenå¤§å°åˆ†å‰²æ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆï¼šåŸºäºå­—ç¬¦æ•°ï¼‰
        
        Args:
            content: è¦åˆ†å‰²çš„æ–‡æœ¬
            overlap_token_size: é‡å çš„tokenæ•°é‡
            max_token_size: æœ€å¤§tokenæ•°é‡
        
        Returns:
            åˆ†å—ç»“æœåˆ—è¡¨
        """
        # å°†tokenå¤§å°è½¬æ¢ä¸ºå­—ç¬¦å¤§å°
        max_chars = max_token_size * self.chars_per_token
        overlap_chars = overlap_token_size * self.chars_per_token
        
        results = []
        start = 0
        chunk_index = 0
        
        while start < len(content):
            # è®¡ç®—å½“å‰å—çš„ç»“æŸä½ç½®
            end = min(start + max_chars, len(content))
            
            # æå–æ–‡æœ¬å—
            chunk_content = content[start:end].strip()
            
            if chunk_content:
                # ä¼°ç®—tokenæ•°é‡
                estimated_tokens = len(chunk_content) // self.chars_per_token + 1
                
                results.append({
                    "tokens": min(max_token_size, estimated_tokens),
                    "content": chunk_content,
                    "chunk_order_index": chunk_index,
                })
                
                chunk_index += 1
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            if end >= len(content):
                break
            
            start = end - overlap_chars
            if start <= 0:
                start = end
        
        return results
