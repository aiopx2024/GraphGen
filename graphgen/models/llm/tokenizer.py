from dataclasses import dataclass
from typing import List, Dict, Tuple
import os
import re
import warnings

# ç®€åŒ–çš„å†…ç½‘ç‰ˆæœ¬ï¼Œå»æ‰å¤æ‚çš„transformersä¾èµ–
# ä½¿ç”¨åŸºæœ¬çš„å­—ç¬¦è®¡æ•°å’Œåˆ†å‰²é€»è¾‘

class SimpleTokenizer:
    """ç®€åŒ–çš„tokenizerï¼Œé€‚ç”¨äºå†…ç½‘ç¯å¢ƒï¼Œæ— éœ€å¤æ‚çš„æ¨¡å‹ä¸‹è½½"""

    def __init__(self, chars_per_token: int = 4):
        """
        ç®€å•çš„tokenizerï¼ŒåŸºäºå­—ç¬¦æ•°ä¼°ç®—tokenæ•°é‡

        Args:
            chars_per_token: å¹³å‡æ¯ä¸ªtokençš„å­—ç¬¦æ•°ï¼ˆä¸­æ–‡çº¦2-3ï¼Œè‹±æ–‡çº¦4-5ï¼‰
        """
        self.chars_per_token = chars_per_token

        # è¯­è¨€æ„ŸçŸ¥çš„tokenæ¯”ä¾‹
        self.lang_ratios = {
            'zh': 1.8,   # ä¸­æ–‡ï¼š1.8å­—ç¬¦/token
            'en': 4.2,   # è‹±æ–‡ï¼š4.2å­—ç¬¦/token
            'mixed': 2.8 # æ··åˆæ–‡æœ¬ï¼š2.8å­—ç¬¦/token
        }

    def detect_language_ratio(self, text: str) -> float:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€æ¯”ä¾‹ï¼ŒåŠ¨æ€è°ƒæ•´tokenä¼°ç®—"""
        if not text.strip():
            return self.lang_ratios['mixed']

        # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(text.strip())

        if total_chars == 0:
            return self.lang_ratios['mixed']

        zh_ratio = chinese_chars / total_chars

        # æ ¹æ®ä¸­æ–‡æ¯”ä¾‹é€‰æ‹©é€‚å½“çš„ä¼°ç®—æ¯”ä¾‹
        if zh_ratio > 0.7:
            return self.lang_ratios['zh']
        elif zh_ratio < 0.3:
            return self.lang_ratios['en']
        else:
            # æ··åˆæ–‡æœ¬ï¼šæ ¹æ®å®é™…æ¯”ä¾‹åŠ æƒå¹³å‡
            return (self.lang_ratios['zh'] * zh_ratio +
                   self.lang_ratios['en'] * (1 - zh_ratio))

    def encode(self, text: str) -> List[int]:
        """ç®€å•ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå­—ç¬¦ç´¢å¼•åˆ—è¡¨"""
        return [ord(c) for c in text]

    def decode(self, tokens: List[int]) -> str:
        """ç®€å•è§£ç ï¼šå°†å­—ç¬¦ç´¢å¼•åˆ—è¡¨è½¬æ¢å›æ–‡æœ¬"""
        return ''.join(chr(token) for token in tokens if 0 <= token <= 1114111)

def get_tokenizer(tokenizer_name: str = "simple"):
    """
    è·å–ç®€åŒ–çš„tokenizerå®ä¾‹
    å†…ç½‘ç‰ˆæœ¬ï¼šä½¿ç”¨ç®€å•çš„å­—ç¬¦åˆ†å‰²ï¼Œæ— éœ€ä¸‹è½½æ¨¡å‹
    """
    print(f"ğŸ”§ ä½¿ç”¨ç®€åŒ–tokenizer: {tokenizer_name}")
    return SimpleTokenizer()

@dataclass
class Tokenizer:
    """æ”¹è¿›çš„Tokenizerç±»ï¼Œæ”¯æŒè¯­è¨€æ„ŸçŸ¥çš„åˆ‡åˆ†ç­–ç•¥"""
    model_name: str = "simple"

    def __post_init__(self):
        self.tokenizer = get_tokenizer(self.model_name)

        # è¯­ä¹‰è¾¹ç•Œæ ‡è®°
        self.sentence_boundaries = {
            'zh': ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›'],
            'en': ['.', '!', '?', ';'],
            'common': ['\n\n', '\n', '\t']
        }

        # æ®µè½è¾¹ç•Œæ ‡è®°
        self.paragraph_boundaries = ['\n\n', '\r\n\r\n']

    def estimate_tokens(self, text: str, language_aware: bool = True) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡

        Args:
            text: è¦ä¼°ç®—çš„æ–‡æœ¬
            language_aware: æ˜¯å¦å¯ç”¨è¯­è¨€æ„ŸçŸ¥ä¼°ç®—

        Returns:
            ä¼°ç®—çš„tokenæ•°é‡
        """
        if not text.strip():
            return 0

        if language_aware:
            chars_per_token = self.tokenizer.detect_language_ratio(text)
        else:
            chars_per_token = 4.0  # é»˜è®¤ä¼°ç®—

        return max(1, int(len(text) / chars_per_token))

    def encode_string(self, text: str) -> List[int]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºtokenåˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼šåŸºäºå­—ç¬¦æ•°ä¼°ç®—ï¼‰
        """
        return self.tokenizer.encode(text)

    def decode_tokens(self, tokens: List[int]) -> str:
        """
        å°†tokenåˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬
        """
        return self.tokenizer.decode(tokens)

    def split_by_semantic_boundaries(self, text: str, boundary_markers: List[str] = None) -> List[str]:
        """
        æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å‰²æ–‡æœ¬

        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            boundary_markers: è‡ªå®šä¹‰è¾¹ç•Œæ ‡è®°

        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬æ®µåˆ—è¡¨
        """
        if not text.strip():
            return []

        # ä½¿ç”¨é»˜è®¤è¾¹ç•Œæ ‡è®°æˆ–è‡ªå®šä¹‰æ ‡è®°
        if boundary_markers is None:
            markers = (
                self.sentence_boundaries['zh'] +
                self.sentence_boundaries['en'] +
                self.sentence_boundaries['common']
            )
        else:
            markers = boundary_markers

        # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text.strip())

        segments = []
        for para in paragraphs:
            if not para.strip():
                continue

            # æŒ‰å¥å­åˆ†å‰²æ®µè½
            # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            pattern = '|'.join(re.escape(marker) for marker in markers if marker not in ['\n\n', '\n', '\t'])
            if pattern:
                sentences = re.split(f'({pattern})', para)
                current_sentence = ""

                for part in sentences:
                    if part.strip():
                        current_sentence += part
                        if any(part.endswith(marker) for marker in markers):
                            if current_sentence.strip():
                                segments.append(current_sentence.strip())
                                current_sentence = ""

                # æ·»åŠ å‰©ä½™éƒ¨åˆ†
                if current_sentence.strip():
                    segments.append(current_sentence.strip())
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥å­è¾¹ç•Œï¼Œç›´æ¥æ·»åŠ æ®µè½
                segments.append(para.strip())

        return [seg for seg in segments if seg.strip()]

    def chunk_by_token_size(
        self,
        content: str,
        overlap_token_size: int = 128,
        max_token_size: int = 1024,
        strategy: str = "semantic",
        preserve_boundaries: bool = True,
        min_chunk_size: int = 100,
        language_aware: bool = True,
        boundary_markers: List[str] = None
    ) -> List[Dict]:
        """
        æ”¹è¿›çš„æ–‡æœ¬åˆ†å—æ–¹æ³•ï¼Œæ”¯æŒå¤šç§ç­–ç•¥

        Args:
            content: è¦åˆ†å‰²çš„æ–‡æœ¬
            overlap_token_size: é‡å çš„tokenæ•°é‡
            max_token_size: æœ€å¤§tokenæ•°é‡
            strategy: åˆ‡åˆ†ç­–ç•¥ ("simple", "semantic", "hierarchical")
            preserve_boundaries: æ˜¯å¦ä¿æŒè¯­ä¹‰è¾¹ç•Œ
            min_chunk_size: æœ€å°chunkå¤§å°
            language_aware: æ˜¯å¦å¯ç”¨è¯­è¨€æ„ŸçŸ¥
            boundary_markers: è‡ªå®šä¹‰è¾¹ç•Œæ ‡è®°

        Returns:
            åˆ†å—ç»“æœåˆ—è¡¨
        """
        if not content.strip():
            return []

        if strategy == "simple":
            return self._simple_chunk(content, overlap_token_size, max_token_size, language_aware)
        elif strategy == "semantic":
            return self._semantic_chunk(
                content, overlap_token_size, max_token_size,
                preserve_boundaries, min_chunk_size, language_aware, boundary_markers
            )
        elif strategy == "hierarchical":
            return self._hierarchical_chunk(
                content, overlap_token_size, max_token_size,
                preserve_boundaries, min_chunk_size, language_aware, boundary_markers
            )
        else:
            warnings.warn(f"Unknown strategy '{strategy}', falling back to 'semantic'")
            return self._semantic_chunk(
                content, overlap_token_size, max_token_size,
                preserve_boundaries, min_chunk_size, language_aware, boundary_markers
            )

    def _simple_chunk(self, content: str, overlap_token_size: int, max_token_size: int, language_aware: bool) -> List[Dict]:
        """ç®€å•æ»‘åŠ¨çª—å£åˆ‡åˆ†"""
        if language_aware:
            chars_per_token = self.tokenizer.detect_language_ratio(content)
        else:
            chars_per_token = 4.0

        max_chars = int(max_token_size * chars_per_token)
        overlap_chars = int(overlap_token_size * chars_per_token)

        results = []
        start = 0
        chunk_index = 0

        while start < len(content):
            end = min(start + max_chars, len(content))
            chunk_content = content[start:end].strip()

            if chunk_content:
                estimated_tokens = self.estimate_tokens(chunk_content, language_aware)
                results.append({
                    "tokens": min(max_token_size, estimated_tokens),
                    "content": chunk_content,
                    "chunk_order_index": chunk_index,
                })
                chunk_index += 1

            if end >= len(content):
                break
            start = max(end - overlap_chars, start + 1)

        return results

    def _semantic_chunk(self, content: str, overlap_token_size: int, max_token_size: int,
                       preserve_boundaries: bool, min_chunk_size: int, language_aware: bool,
                       boundary_markers: List[str]) -> List[Dict]:
        """åŸºäºè¯­ä¹‰è¾¹ç•Œçš„åˆ‡åˆ†"""
        if preserve_boundaries:
            segments = self.split_by_semantic_boundaries(content, boundary_markers)
        else:
            segments = [content]

        results = []
        current_chunk = []
        current_tokens = 0
        chunk_index = 0

        for segment in segments:
            segment_tokens = self.estimate_tokens(segment, language_aware)

            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œéœ€è¦å¼ºåˆ¶åˆ†å‰²
            if segment_tokens > max_token_size:
                # å…ˆå¤„ç†å½“å‰ç´¯ç§¯çš„chunk
                if current_chunk and current_tokens >= min_chunk_size:
                    chunk_content = ' '.join(current_chunk)
                    results.append({
                        "tokens": current_tokens,
                        "content": chunk_content,
                        "chunk_order_index": chunk_index,
                    })
                    chunk_index += 1

                # å¯¹è¶…å¤§æ®µè½è¿›è¡Œå¼ºåˆ¶åˆ†å‰²
                large_chunks = self._simple_chunk(segment, overlap_token_size, max_token_size, language_aware)
                for chunk in large_chunks:
                    chunk["chunk_order_index"] = chunk_index
                    results.append(chunk)
                    chunk_index += 1

                current_chunk = []
                current_tokens = 0
                continue

            # æ£€æŸ¥åŠ å…¥å½“å‰æ®µè½æ˜¯å¦è¶…è¿‡é™åˆ¶
            if current_tokens + segment_tokens > max_token_size:
                if current_chunk and current_tokens >= min_chunk_size:
                    chunk_content = ' '.join(current_chunk)
                    results.append({
                        "tokens": current_tokens,
                        "content": chunk_content,
                        "chunk_order_index": chunk_index,
                    })
                    chunk_index += 1

                # å¤„ç†é‡å ï¼šä»å½“å‰chunkçš„æœ«å°¾å¼€å§‹æ–°chunk
                if overlap_token_size > 0 and current_chunk:
                    overlap_segments = []
                    overlap_tokens = 0

                    # ä»åå¾€å‰é€‰æ‹©æ®µè½ä½œä¸ºé‡å å†…å®¹
                    for i in range(len(current_chunk) - 1, -1, -1):
                        seg_tokens = self.estimate_tokens(current_chunk[i], language_aware)
                        if overlap_tokens + seg_tokens <= overlap_token_size:
                            overlap_segments.insert(0, current_chunk[i])
                            overlap_tokens += seg_tokens
                        else:
                            break

                    current_chunk = overlap_segments
                    current_tokens = overlap_tokens
                else:
                    current_chunk = []
                    current_tokens = 0

            current_chunk.append(segment)
            current_tokens += segment_tokens

        # å¤„ç†å‰©ä½™å†…å®¹
        if current_chunk and current_tokens >= min_chunk_size:
            chunk_content = ' '.join(current_chunk)
            results.append({
                "tokens": current_tokens,
                "content": chunk_content,
                "chunk_order_index": chunk_index,
            })

        return results

    def _hierarchical_chunk(self, content: str, overlap_token_size: int, max_token_size: int,
                          preserve_boundaries: bool, min_chunk_size: int, language_aware: bool,
                          boundary_markers: List[str]) -> List[Dict]:
        """å±‚æ¬¡åŒ–åˆ‡åˆ†ï¼šæ–‡æ¡£->æ®µè½->å¥å­"""
        # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', content.strip())

        results = []
        chunk_index = 0

        for para in paragraphs:
            if not para.strip():
                continue

            para_tokens = self.estimate_tokens(para, language_aware)

            if para_tokens <= max_token_size:
                # æ®µè½é€‚åˆå•ä¸ªchunk
                if para_tokens >= min_chunk_size:
                    results.append({
                        "tokens": para_tokens,
                        "content": para.strip(),
                        "chunk_order_index": chunk_index,
                    })
                    chunk_index += 1
            else:
                # æ®µè½éœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                para_chunks = self._semantic_chunk(
                    para, overlap_token_size, max_token_size,
                    preserve_boundaries, min_chunk_size, language_aware, boundary_markers
                )
                for chunk in para_chunks:
                    chunk["chunk_order_index"] = chunk_index
                    results.append(chunk)
                    chunk_index += 1

        return results
