#!/usr/bin/env python3
"""
GraphGenæ‰¹é‡å¤„ç†å·¥å…·
ç”¨äºæ‰¹é‡å¤„ç†å¤§é‡txtæ–‡ä»¶ï¼Œç”ŸæˆçŸ¥è¯†å›¾è°±å’Œå„ç§ç±»å‹çš„è¯­æ–™å¯¹

åŠŸèƒ½ï¼š
1. æ‰¹é‡å¤„ç†å¤šä¸ªtxtæ–‡ä»¶
2. ç”ŸæˆGraphMLçŸ¥è¯†å›¾è°±æ–‡ä»¶
3. ç”Ÿæˆä¸åŒç±»å‹çš„QAè¯­æ–™å¯¹ï¼ˆatomic, aggregated, multi_hop, cotï¼‰
4. æ”¯æŒè‡ªå®šä¹‰é…ç½®å’Œè¾“å‡ºç›®å½•
"""

import os
import json
import argparse
import time
from pathlib import Path
from typing import List, Dict, Any
import yaml
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from graphgen.graphgen import GraphGen
from graphgen.utils import logger, set_logger


def prepare_txt_data(txt_files: List[str], chunk_size: int = 512) -> List[Dict[str, Any]]:
    """
    å‡†å¤‡txtæ–‡ä»¶æ•°æ®ï¼Œè½¬æ¢ä¸ºGraphGenå¯å¤„ç†çš„æ ¼å¼
    
    Args:
        txt_files: txtæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
    
    Returns:
        å¤„ç†åçš„æ•°æ®åˆ—è¡¨
    """
    data = []
    
    for txt_file in txt_files:
        print(f"å¤„ç†æ–‡ä»¶: {txt_file}")
        
        with open(txt_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            
        # å¦‚æœå†…å®¹è¿‡é•¿ï¼Œè¿›è¡Œåˆ†å—
        if len(content) > chunk_size:
            chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]
            for i, chunk in enumerate(chunks):
                data.append({
                    "content": chunk.strip(),
                    "source_file": txt_file,
                    "chunk_id": i
                })
        else:
            data.append({
                "content": content,
                "source_file": txt_file,
                "chunk_id": 0
            })
    
    return data


def create_config(output_type: str, input_file: str, config_template: str = None) -> Dict[str, Any]:
    """
    åˆ›å»ºGraphGené…ç½®
    
    Args:
        output_type: è¾“å‡ºç±»å‹ (atomic, aggregated, multi_hop, cot)
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        config_template: é…ç½®æ¨¡æ¿è·¯å¾„
    
    Returns:
        é…ç½®å­—å…¸
    """
    # åŸºç¡€é…ç½®
    config = {
        "input_data_type": "raw",
        "input_file": input_file,
        "output_data_type": output_type,
        "output_data_format": "ChatML",
        "tokenizer": "cl100k_base",
        "search": {
            "enabled": False,
            "search_types": ["wikipedia"]
        },
        "quiz_and_judge_strategy": {
            "enabled": True,
            "quiz_samples": 2,
            "re_judge": False
        }
    }
    
    # æ ¹æ®è¾“å‡ºç±»å‹è®¾ç½®éå†ç­–ç•¥
    if output_type == "atomic":
        config["traverse_strategy"] = {
            "bidirectional": True,
            "edge_sampling": "max_loss",
            "expand_method": "max_tokens",
            "isolated_node_strategy": "ignore",
            "max_depth": 2,
            "max_tokens": 128,
            "loss_strategy": "only_edge"
        }
    elif output_type == "aggregated":
        config["traverse_strategy"] = {
            "bidirectional": True,
            "edge_sampling": "max_loss",
            "expand_method": "max_width",
            "isolated_node_strategy": "ignore",
            "max_depth": 5,
            "max_extra_edges": 20,
            "max_tokens": 256,
            "loss_strategy": "only_edge"
        }
    elif output_type == "multi_hop":
        config["traverse_strategy"] = {
            "bidirectional": True,
            "edge_sampling": "max_loss",
            "expand_method": "max_tokens",
            "isolated_node_strategy": "ignore",
            "max_depth": 3,
            "max_tokens": 512,
            "loss_strategy": "both"
        }
    elif output_type == "cot":
        config["method_params"] = {
            "method": "leiden",
            "num_communities": 10,
            "max_samples_per_community": 5
        }
    
    return config


def process_single_batch(txt_files: List[str], output_dir: str, output_types: List[str], 
                        chunk_size: int = 512, enable_trainee: bool = True):
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶
    
    Args:
        txt_files: txtæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        output_types: è¾“å‡ºç±»å‹åˆ—è¡¨
        chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
        enable_trainee: æ˜¯å¦å¯ç”¨traineeæ¨¡å‹
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    batch_dir = os.path.join(output_dir, f"batch_{int(time.time())}")
    os.makedirs(batch_dir, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®
    print("å‡†å¤‡è¾“å…¥æ•°æ®...")
    data = prepare_txt_data(txt_files, chunk_size)
    
    # ä¿å­˜åŸå§‹æ•°æ®ä¸ºjsonlæ ¼å¼
    input_file = os.path.join(batch_dir, "input_data.jsonl")
    with open(input_file, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"å·²ä¿å­˜è¾“å…¥æ•°æ®åˆ°: {input_file}")
    print(f"æ€»å…±å¤„ç†äº† {len(data)} ä¸ªæ–‡æœ¬å—")
    
    # ä¸ºæ¯ç§è¾“å‡ºç±»å‹ç”Ÿæˆè¯­æ–™
    for output_type in output_types:
        print(f"\nå¼€å§‹ç”Ÿæˆ {output_type} ç±»å‹çš„è¯­æ–™...")
        
        try:
            # åˆ›å»ºé…ç½®
            config = create_config(output_type, input_file)
            
            # ç¦ç”¨traineeç›¸å…³åŠŸèƒ½ï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
            if not enable_trainee:
                config["quiz_and_judge_strategy"]["enabled"] = False
                if "traverse_strategy" in config:
                    config["traverse_strategy"]["edge_sampling"] = "random"
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            type_output_dir = os.path.join(batch_dir, output_type)
            os.makedirs(type_output_dir, exist_ok=True)
            
            # è®¾ç½®æ—¥å¿—
            log_file = os.path.join(type_output_dir, f"graphgen_{output_type}.log")
            set_logger(log_file, if_stream=True)
            
            # åˆå§‹åŒ–GraphGen
            graph_gen = GraphGen(config=config, working_dir=type_output_dir)
            
            # æ‰§è¡Œç”Ÿæˆæµç¨‹
            if output_type == "cot":
                # CoTç”Ÿæˆæµç¨‹
                graph_gen.insert()
                if config["search"]["enabled"]:
                    graph_gen.search()
                graph_gen.generate_reasoning(method_params=config["method_params"])
            else:
                # å¸¸è§„QAç”Ÿæˆæµç¨‹
                graph_gen.insert()
                
                if config["search"]["enabled"]:
                    graph_gen.search()
                
                if config["quiz_and_judge_strategy"]["enabled"]:
                    graph_gen.quiz()
                    graph_gen.judge_statements()
                else:
                    graph_gen.traverse_strategy.edge_sampling = "random"
                
                graph_gen.traverse()
            
            # ä¿å­˜GraphMLæ–‡ä»¶
            graphml_path = os.path.join(type_output_dir, f"knowledge_graph_{output_type}.graphml")
            await_graph = graph_gen.graph_storage.get_graph()
            if await_graph:
                import asyncio
                loop = asyncio.get_event_loop()
                graph = loop.run_until_complete(await_graph)
                import networkx as nx
                nx.write_graphml(graph, graphml_path)
                print(f"GraphMLå·²ä¿å­˜åˆ°: {graphml_path}")
            
            # ä¿å­˜é…ç½®æ–‡ä»¶
            config_path = os.path.join(type_output_dir, f"config_{output_type}.yaml")
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            print(f"âœ… {output_type} ç±»å‹è¯­æ–™ç”Ÿæˆå®Œæˆ")
            print(f"è¾“å‡ºç›®å½•: {type_output_dir}")
            
        except Exception as e:
            print(f"âŒ {output_type} ç±»å‹è¯­æ–™ç”Ÿæˆå¤±è´¥: {str(e)}")
            logger.error(f"Error generating {output_type}: {str(e)}")


def main():
    parser = argparse.ArgumentParser(
        description="GraphGenæ‰¹é‡å¤„ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  # å¤„ç†å•ä¸ªç›®å½•ä¸‹çš„æ‰€æœ‰txtæ–‡ä»¶
  python batch_process.py --input-dir /path/to/txt/files --output-dir results
  
  # å¤„ç†æŒ‡å®šçš„txtæ–‡ä»¶ï¼Œç”Ÿæˆç‰¹å®šç±»å‹çš„è¯­æ–™
  python batch_process.py --input-files file1.txt file2.txt --output-dir results --types atomic aggregated
  
  # ç¦ç”¨traineeæ¨¡å‹ï¼ˆä»…ä½¿ç”¨synthesizerï¼‰
  python batch_process.py --input-dir /path/to/txt/files --output-dir results --no-trainee
        """
    )
    
    # è¾“å…¥é€‰é¡¹
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('--input-dir', help='åŒ…å«txtæ–‡ä»¶çš„ç›®å½•è·¯å¾„')
    input_group.add_argument('--input-files', nargs='+', help='æŒ‡å®šçš„txtæ–‡ä»¶è·¯å¾„åˆ—è¡¨')
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--types', nargs='+', 
                       choices=['atomic', 'aggregated', 'multi_hop', 'cot'],
                       default=['atomic', 'aggregated', 'multi_hop'],
                       help='è¦ç”Ÿæˆçš„è¯­æ–™ç±»å‹')
    
    # å¤„ç†é€‰é¡¹
    parser.add_argument('--chunk-size', type=int, default=512, help='æ–‡æœ¬åˆ†å—å¤§å°')
    parser.add_argument('--batch-size', type=int, default=10, help='æ¯æ‰¹æ¬¡å¤„ç†çš„æ–‡ä»¶æ•°é‡')
    parser.add_argument('--no-trainee', action='store_true', help='ç¦ç”¨traineeæ¨¡å‹')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_vars = ["SYNTHESIZER_MODEL", "SYNTHESIZER_BASE_URL", "SYNTHESIZER_API_KEY"]
    if not args.no_trainee:
        required_vars.extend(["TRAINEE_MODEL", "TRAINEE_BASE_URL", "TRAINEE_API_KEY"])
    
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨.envæ–‡ä»¶")
        return
    
    # è·å–txtæ–‡ä»¶åˆ—è¡¨
    if args.input_dir:
        txt_files = list(Path(args.input_dir).glob("*.txt"))
        txt_files = [str(f) for f in txt_files]
    else:
        txt_files = args.input_files
    
    if not txt_files:
        print("âŒ æœªæ‰¾åˆ°txtæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶")
    print(f"å°†ç”Ÿæˆçš„è¯­æ–™ç±»å‹: {', '.join(args.types)}")
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(txt_files), args.batch_size):
        batch_files = txt_files[i:i + args.batch_size]
        print(f"\nå¤„ç†æ‰¹æ¬¡ {i//args.batch_size + 1}/{(len(txt_files)-1)//args.batch_size + 1}")
        print(f"åŒ…å«æ–‡ä»¶: {len(batch_files)} ä¸ª")
        
        process_single_batch(
            batch_files, 
            args.output_dir, 
            args.types,
            args.chunk_size,
            not args.no_trainee
        )
    
    print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")


if __name__ == "__main__":
    main()